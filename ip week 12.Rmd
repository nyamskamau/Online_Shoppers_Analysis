---
title: "Independent Project Week 12"
author: "Roselynn"
date: "1/8/2021"
output: html_document
---


### Defining the Question 

To identify the individuals that are more likely to click on the ads.

### The Metric for Success

To provide an accurate picture of the people most likely to view the clients advertisements and provide recommendations to the client based on the results of this analysis.

### The Context
A Kenyan entrepreneur has created an online cryptography course and would want to advertise it on her blog. She currently targets audiences originating from various countries. In the past, she ran ads to advertise a related course on the same blog and collected data in the process. She would now like to employ your services as a Data Science Consultant to help her identify which individuals are most likely to click on her ads

### The Experimental Design Taken

For this analysis I first loaded the dataset provided , previewed the dataset and carried out data cleaning. Next I checked for anomalies and outliers in the dataset and dealt with these. 
Finally I carried out Univariate and Bivariate analysis and provided insights and recommendations to the client based on my findings.

### Appropriateness of the data 

Given the task at hand the data provided is appropriate to carry out this analysis.


# 1. Loading the Appropriate Libraries and Importing the Dataset

I loaded the tidyverse library onto my editor  and the I imported the dataset and previewed it

``` {r}
library(tidyverse)
```

I then imported the dataset and previewed the top and bottom columns in the dataset.

```{r}
#Importing  the dataset
advertising <- read_csv("C:/Users/Njeri/Downloads/advertising.csv")
View(advertising)
```


``` {r}
# Previewing the dataset
head(advertising)
tail(advertising)
```

The columns in the dataset were :

```{r}
names(advertising)
```

The descriptions of these columns was as follows:

1. 'Daily Time Spent on Site': consumer time on site in minutes
2. 'Age': customer age in years
3. 'Area Income': Avg. Income of geographical area of consumer
4. 'Daily Internet Usage': Avg. minutes a day consumer is on the internet
5. 'Ad Topic Line': Headline of the advertisement
6. 'City': City of consumer
7. 'Male': Whether or not consumer was male
8. 'Country': Country of consumer
9. 'Timestamp': Time at which consumer clicked on Ad or closed window
10. 'Clicked on Ad': 0 or 1 indicated clicking on Ad



I then checked for the data types of the columns in the dataset.
The dataset had 1,000 rows and 10 columns , with three being of type character , one of type datetime and the others being numeric.
The Numeric variables had four continuous and two categorical variables.

```{r}
glimpse(advertising)
```

I then used the summary() function to obtain the summary statistics of the columns in the dataset.

```{r}
summary(advertising)

```

# 2. Data Cleaning

The first step of my data cleaning process was checking for any null entries in the dataset using the colSums and is.na functions.
The dataset had no null entries.

```{r}
colSums(is.na(advertising))
```

Next I checked for any duplicated entries in the dataset.
I deduced that there were none.

``` {r}
sum(duplicated(advertising))
```

Given that the dataset had neither null nor duplicated values I carried out my exploratory data analysis.


# 3. Exploratory Data Analysis

### 3.1. Univariate Data Analysis

For my univariate data analysis I opted to analyze all the columns individually to allow me to be able to pay close attention to my findings.

``` {r echo=T}
time <- advertising$`Daily Time Spent on Site`
age <- advertising$Age
income <-  advertising$`Area Income`
usage <- advertising$`Daily Internet Usage`
```

I used the moments package in R to check for the skewness and kurtosis of the columns in the dataset.

```{r}
#install.packages('moments')
library(moments)
```

#### Daily Time Spent on the Site
As per the column descriptions , this column recorded the daily amount of time users spent on the site.

``` {r}
summary(time)
# Calculating the mode
getmode <- function(v) #getmode is the function name
  {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
# Calculate the mode using the user function.
mode <- getmode(time)
print(mode)
# The Standard Deviation and Variance
var(time)
sd(time)
#The range of the Variable
range(time)
#The Interquartile Range
IQR(age)
#The Skew and Kurtosis of the column
skewness(time)
#The Kurtosis
kurtosis(time)
# Histogram with density plot
ggplot(advertising, aes(x=`Daily Time Spent on Site`)) + 
 geom_histogram(colour="black", fill="grey",bins=10)#+
 #geom_density(alpha=.2, fill="#FF6666") 
# PLotting a boxplot to check for outliers
ggplot(advertising, aes(x=`Daily Time Spent on Site`)) + geom_boxplot(outlier.color = 'black',fill='grey')

```

 From this column I observed that :
 
  * The mean amount of time that users spent on the site was 65 minutes while the maximum amount of time a user spent on the site was 91.43 minutes.
  * The Variance of the column was 251.3371 with a standard deviation of 15.85361.
  * The data was negatively but fairly symmetrical with a value of -0.3712026 and the
  distribution can be categorized as platykurtic with a kurtosis value of 1.903942
  * Furthermore plotting a boxplot of the variable I observed that it didnot have any outliers .
  * Plotting a histogram for the column we can see that approxiamtely 125 users spent over 80 minutes daily on the site , and many users spent over 60 minutes on the site daily.


#### Age

``` {r}
summary(age)
# Calculating the mode
getmode <- function(v) #getmode is the function name
  {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
# Calculate the mode using the user function.
mode <- getmode(age)
print(mode)
# The Standard Deviation and Variance
var(age)
sd(age)
#The range of the Variable
range(age)
#The interquartile range
IQR(age)
#The Skew and Kurtosis of the column
skewness(age)
#The Kurtosis
kurtosis(age)
# Histogram with density plot
ggplot(advertising, aes(x=age)) + 
 geom_histogram(colour="black", fill="grey",bins=10)#+
 #geom_density(alpha=.2, fill="#FF6666") 
# PLotting a boxplot to check for outliers
ggplot(advertising, aes(x=age)) + geom_boxplot(outlier.color = 'black',fill='grey')

```

From the Age column I observed that :

* The mean age of the consumers was 36.01 while the median age was 35 and the modal age was 31.
* The variable was positively skewed and fairly symmetrical with a skew value of  0.4777052 , the distribution was platykurtic with a kurtosis value of  2.595482
* The age of the consumers ranged between 19 and 61 , with majority of the users ranging between 30 and 45. The interquartile age for the upper and lower quartile was 13.
* The were no outliers in this column as observed from the boxplot. 

#### Area Income Variable

``` {r}
summary(income)
# Calculating the mode
getmode <- function(v) #getmode is the function name
  {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
## Calculate the mode using the user function.
mode <- getmode(income)
print(mode)
# The Standard Deviation and Variance
var(income)
sd(income)
#The range of the Variable
range(income)
#The Interquartile range
IQR(income)
#The Skew and Kurtosis of the column
skewness(income)
#The Kurtosis 
kurtosis(income)
# Histogram with density plot
ggplot(advertising, aes(x=income)) + 
 geom_histogram(colour="black", fill="grey",bins=15)#+
 #geom_density(alpha=.2, fill="#FF6666") 
# PLotting a boxplot to check for outliers
ggplot(advertising, aes(x=income)) + geom_boxplot(fill='grey',outlier.color = 'black')

```

From the Income column we can deduce that :

* The median income of the users was 57,012 while the mean income was 55,000 and the modal income was 61,833.9.
* The income of the site users ranged from 13,996 to 79,485 , with the interquarile range being 18483.83.
* The data had a negative skew and was moderately skewed with a value of -0.6484229,
the data had a platykurtic distribution with a value of 2.894694.
* There were several outliers in the column 
 

####  Daily Internet Usage Variable

``` {r}
summary(usage)
# Calculating the mode
getmode <- function(v) #getmode is the function name
  {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
# Calculate the mode using the user function.
mode <- getmode(usage)
print(mode)
# The Standard Deviation and Variance
var(usage)
sd(usage)
#The range of the Variable
range(usage)
#The Interquartile Range
IQR(usage)
#The Skew and Kurtosis of the column
skewness(usage)
# Kurtosis
kurtosis(usage)
# Histogram with density plot
ggplot(advertising, aes(x=usage)) + 
 geom_histogram(colour="black", fill="grey",bins=10)#+
 #geom_density(alpha=.2, fill="#FF6666") 
# PLotting a boxplot to check for outliers
ggplot(advertising, aes(x=usage)) + geom_boxplot(outlier.color = 'black',fill='grey')

```

From the daily internet usage we can see that:

* The Average Hours spent by users on the Internet is 180 minutes while the median is 183.1 and the mode is 167.22.
* The Interquartile Range is 79.9625.
* The data is negatively but fairly skewed with a skew value of -0.03343681 and the data is platykurtic with a value of  1.727701
* There were no outliers in this column as observed in the boxplot plotted for this column.


#### Plotting Countplots for the Categorical variables.

**I then plotted countplots for the categorical data in the dataset and observed that:**


```{r}
ggplot(advertising, aes(x=Male)) + geom_bar(fill=rgb(0.4,0.1,0.5))

```

There were more non-male than male users who visited the site.


```{r}
ggplot(advertising, aes(x=factor(`Clicked on Ad`))) + geom_bar( fill=rgb(0.4,0.1,0.5))

```

The number of users on the site who clicked on the ad is equal to those that did not.



### 3.2. Bivariate Data Analysis

I plotted scatter plots for the numeric variables in the dataset

```{r}
ggplot(advertising, aes(x=income , y = age )) + geom_point(aes(colour= factor(`Clicked on Ad`)))

``` 

+ The scatter plot for the Area Income against Age showed that , majority of the users who did not click on the ad were the high income earners and many of these were aged between 20 and 40 years.

+ This is a particularly interesting fact , a survey by the WebStrategies company that showed that high income earners are less tolerant to ads thus less likely to click on  banner or mobile ads on the internet. [How Does Income Level Affect Online Behavior](https://www.webstrategiesinc.com/blog/how-does-income-level-affect-online-behavior-in-central-virginia)

Next I plotted a scatter plot for the Income and Time, time spent of the site i.e, variables.

```{r}
ggplot(advertising, aes(x=income , y = time )) + geom_point(aes(colour= factor(`Clicked on Ad`)))

``` 

+ Once more I noted that the people who were least likely to click on the ad were the higher income earners , this was despite the fact that they seemed to spend a over an hour a day on the site.

+ The same sentiments can be echoed for the Usage , total internet usage per day , variable . When plotted against income we see that those who spend over 200 minutes online all day and earn more than 50,000 are the least likely to click on ads on the internet.

```{r}
ggplot(advertising, aes(x=income , y = usage)) + geom_point(aes(colour= factor(`Clicked on Ad`)))

```


```{r}
ggplot(advertising, aes(x = age , y = time )) + geom_point(aes(colour= factor(`Clicked on Ad`)))

```

+ Further plotting the Age against Time spent on the site variable we see that the younger demographic are less tolerant to ads despite spending significant amounts of time on the site .

+ The reason for this may be that younger people , are more tech savvy and therefore are more likely to detect ads and avoid them while using the internet compared to their older counterparts.


+ Similar behavior can be seen for the Amount of time spent by the users online.According to the article linked here [Baby Boomers Far More Likely to Click Online Ads ](https://www.prnewswire.com/news-releases/baby-boomers-far-more-likely-to-click-online-ads-than-younger-generations---but-irrelevant-ads-frustrate-everyone-130265733.html)


```{r}
ggplot(advertising, aes(x= age , y = usage )) + geom_point(aes(colour= factor(`Clicked on Ad`)))

```



Finally I plotted a correlation matrix .

+ Plotting a correlation matrix for the numeric variables :

```{r}

# Selecting the Numerical Variables of the dataset
corr <- select(advertising , Age , 'Area Income', `Clicked on Ad`,
               `Daily Internet Usage`, `Daily Time Spent on Site` , Male )


``` 

```{r}

# Plotting the Correlation Heatmap
library(ggcorrplot)
ggcorrplot(cor(corr), hc.order = F,type = 
"lower", lab = T ,
  ggtheme = ggplot2::theme_gray,
  colors = c("#00798c", "white", "#edae49"),tl.srt = 90)

```

+ Here I noted that : 
   
    * There was a strong negative correlation between the Daily Internet usage and Clicked on Ad variables.
    This means that the higher ones income the less likely they are to click on the blog ads. The same can also be said for the Daily Time Spent on Site and Click on ad variables.
    
    * The Click on Ad variable had a strong positive correlation with the Age Variable, the older users were more likely to click on the ad , as we observed above in our analysis.
    
    * The clicked on ad variable was also strongly negatively correlated with the Area Income , where the higher ones income was the less likely they were to click on the ad.


# 4. Model Building and Evaluation.

I first removed the features that I was not using to build my model.
The City ancf Country columns which may have been useful features had too many unique variables and hence I opted to drop them as encoding them would not be a viable option.



```{r}
# Removing the unnecessary features from my dataset
advertising <- select(advertising , c(-City,-Country,-Timestamp,-`Ad Topic Line`))
```



In order to build my model , I used the Janitor library to clean the names of the column for instance the `Clicked on Ad` when passed through the clean_names function becomes clicked_on_ad

```{r}
library(janitor)

advertising <- clean_names(advertising)

names(advertising)
```



## 4.1.Splitting the Data
 
I split my dataset into a training and testing set and then built my model. 
Given that the Target variable , `Clicked on Ad` was binary I built a classification model.
 

```{r}
# Splitting the Dataset into a Train and Test set with an 80:20 split

sample_train<- sample(seq_len(nrow(advertising)), size = floor(0.80*nrow(advertising)))
sample_test <- sample(seq_len(nrow(advertising)), size = floor(0.20*nrow(advertising)))

training     <- advertising[sample_train, ]
test      <- advertising[sample_test, ]

dim(training)
dim(test)
```

My training set after an 80:20 split had 800 rows and 6 columns while the test set had 200 rows with 6 columns.


## 4.2. Training the Model

I then used the as.factor function for my target as the model was a Classification model


Finally I built my decision tree using cross validation to find the optimal model.

```{r}
library(caret)
training$clicked_on_ad <- as.factor(training$clicked_on_ad)
test$clicked_on_ad <- as.factor(test$clicked_on_ad)


trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
set.seed(123)
dtree_fit <- train(clicked_on_ad ~., data = training, method = "rpart",
                   parms = list(split = "information"),
                   trControl=trctrl,
                   tuneLength = 10)


```


```{r}

dtree_fit

```


And then plotted the tree for the optimal model 

```{r}
library(rpart)
library(rpart.plot)
prp(dtree_fit$finalModel, box.palette = "Reds")

```


## 4.3. Model Evaluation 

I evaluated the model using the confusion matrix.

```{r}

test_pred <- predict(dtree_fit, newdata = test)
confusionMatrix(test_pred, test$clicked_on_ad )

```

The model had an accuracy of 0.945 , with 99 true positives , 5 false negatives and 90 true negatives with 6 false positives.


#  5. Conclusions and Recommendations.

+ In analyzing this data I deduced that:
   * Older people , those over 35 were more likely to click on the course ad.
   * The individuals earning higher salaries were more likely not to click on the ad.
   * The probability that a consumer would click on the ad was 0.5.
   * The more time users spent on the blog , the less likely they were to click on the advertisement.
   
+ Thus given these observations I would recommend that:

1. The Blogger should target users who were aged over 35 , as they were more likely to click on the ad. 

2. Furthermore focusing more on those earning a lower income i.e less than 60,000 would prove to be more beneficial as these consumers click on ads more.

3. Finally the users who spend less time on the site and on the internet in general would prove a better demographic for the ads.


